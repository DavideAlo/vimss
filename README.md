# vimss
Visually-informed Music Source Separation project at Jeju 2018 Deep Learning Summer Camp


## Dataset

### Pre-training

1. AudioSet
2. Youtube-8M dataset

### Finetuning and Evaluation 
1. URMP https://datadryad.org//resource/doi:10.5061/dryad.ng3r749
2. Juan's SoP https://drive.google.com/drive/folders/1Gq4iHBAzZfAuM2Gej9gYpBzS3iQP9W0G
3. (not sure) https://github.com/ardasnck/learning_to_localize_sound
4. (not sure) Clarinet4Science dataset

## Baselines

https://github.com/ShichengChen/WaveNetSeparateAudio
https://github.com/MTG/DeepConvSep
https://github.com/interactiveaudiolab/nussl

## Related work

1. Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, Antonio Torralba. Sounds of Pixels
2. Andrew Owens, Alexei A. Efros. Audio-Visual Scene Analysis with Self-Supervised Multisensory Features
3. Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, In So Kweon. Learning to Localize Sound Source in Visual Scenes
4. Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, Michael Rubinstein. Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation
5. Relja Arandjelovic, Andrew Zisserman. Objects that Sound
6. Ruohan Gao, Rogerio Feris, Kristen Grauman. Learning to Separate Object Sounds by Watching Unlabeled Video
7. Sanjeel Parekh, Slim Essid, Alexey Ozerov, Ngoc Q. K. Duong, Patrick Pérez, Gaël Richard. Weakly Supervised Representation Learning for Unsynchronized Audio-Visual Events

## Roadmap
